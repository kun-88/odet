"""Investigate the influence and importance of raw data representations (such as inter-arrival time (IAT) and packet
sizes) of IoT traffic to outlier detection.

Outlier detection algorithms:
    OCSVM, KDE, IF, GMM, PCA, and AE.

Measure metric:
    The area under the ROC curve (AUC).

Experiments:
    Exp 1:
        The AUC results generated by each algorithm with default parameters, i.e. using the rule of thumb
        (empirical rule) as default parameters for each algorithm to obtain the results.
    Exp 2:
        The AUC results generated by each algorithm with best parameters.

Note:
    ***Please do not use "optimize imports" in pycharm to this file, because this function will the imported order
of customized packages or libraries, such as "_config".

Execute:
    # run under "examples"
    cd examples/
    python3 -V
    PYTHONPATH=../:./ python3.7 reprst/main_reprst_srcip.py > out/reprst_srcip_new/main_reprst_srcip.txt 2>&1 &

"""
# Authors: kun.bj@outlook.com
#
# License: xxx

##############################################################################################
# Environment path
import os
import sys

lib_path = os.path.abspath('../')
sys.path.append(lib_path)
# print(os.getcwd())
# os.chdir('examples')
# print(f"add \'{lib_path}\' into sys.path: {sys.path}")

# At the very beginning of the main entrance: "set random seeds and basic parameters to make experimental results
# reproducible.
try:
    # if os.path.exists('_config.py'):
    from representation.default_config import *

    print('from _config_reprst import *')
except:
    from odet._config import *

    print('from itod.config import *')

# 1. System and built-in libraries
import argparse
import traceback
# 2. Third-party libraries
# such as numpy and pyod
# import pretty_errors  # please do not remove it because it helps to locate errors.

# 3. Local libraries
# focus on data processes, which contains pcap2flows, flow2features, and so on.
from odet.pparser.data_factory import DataFactory
# highlight on outlier detection algorithms
from odet.ndm.detector_factory import DetectorFactory
# # highlight on data visual
from odet.visual.display_factory import DisplayFactory
# # include useful tools, such as save data
# include results postprocess, such as results2xlsx and results2latex.
from odet.visual.postprocessing.latex_csv_xlsx import *


class MainFactory:
    """Includes an complete outlier detection procedure of each algorithm with each combination of parameters,
    such as DataFactory, DetectorFactory, and DisplayFactory
    """

    def __init__(self, params={}):
        """Initialize all parameters

        Parameters
        ----------
        params: dict
            all the parameters used in each procedure, and also will be updated during the procedure.

            "params" uses "dict" because so many parameters are includes in params

        """
        self.params = params
        pprint(self.params, name=MainFactory.__name__)  # print common parameters

        self.dataset_name = self.params['dataset_name']
        self.detector_name = self.params['detector_name']

    @func_notation
    def run(self):
        """Execute function
            1. Get features from pcap
            2. Get AUCs
            3. Visual the results (i.e., store to .dat, csv, xlsx, pdf, etc)
        Returns
        -------
        dp.data_lst: list
            AUC results stored in a list

        """
        #############################################################################################
        print("1. Data process")
        self.dt = DataFactory(dataset_name=self.dataset_name, params=self.params)  # update params

        header = self.dt.params['header']
        gs = self.dt.params['gs']
        opt_dir = self.dt.params['ipt_dir']
        dataset_name = self.dt.params['dataset_name']
        output_file = pth.join(opt_dir, f'{dataset_name}/all-features-header:{header}.dat')
        if pth.exists(output_file):
            try:
                with open(output_file, 'rb') as f:
                    self.dt = pickle.load(f)
            except Exception as e:
                traceback.print_tb(e.__traceback__)
        else:
            self.dt.run()  # 1) pcap2features, 2) get train set and test set 3) update params
            if not pth.exists(pth.dirname(output_file)):
                os.makedirs(pth.dirname(output_file))
            with open(output_file, 'wb') as f:
                pickle.dump(self.dt, f)

            # # output_file = './output_data_PCA/header_False-gs_False-fft_iat.dat'
            # output_file = './output_data_PCA/all_features-header_False-gs_False.dat'
            # features.dat will be transform to csv format
            with open(output_file, 'rb') as f:
                self.dt = pickle.load(f)
            transform_to_csv(self.dt.dataset_inst, output_dir=pth.join(opt_dir, f'{dataset_name}'))
        print('features data:', output_file)

        #############################################################################################
        print("2. Algorithm analysis")
        # Execute algorithm and get AUCs
        # Normalization should be done in this part rather than DataFactory because some algorithms might not
        # require normalization
        self.dtr = DetectorFactory(detector_name=self.detector_name, dataset_inst=self.dt.dataset_inst,
                                   params=self.params)  # update params
        self.dtr.run()

        #############################################################################################
        print("3. Result display")
        self.dp = DisplayFactory(dataset_inst=self.dtr.dataset_inst, params=self.params)
        self.dp.run()

        return self.dp.data_lst  # all the results


class Parameter:
    """All experimental parameters

    """

    def __init__(self, random_state=42, verbose=1, display=True, overwrite=False):
        """Common parameters

        Parameters
        ----------
        random_state : int
            make experimental results reproducible

        verbose: int
            need to be modified in the future version: verbose: Verbosity mode, 0, 1 or 2

        display: boolean, default as True
            display figures or not.

        overwrite: boolean, default as False

        """
        self.random_state = random_state
        self.verbose = verbose
        self.display = display
        self.overwrite = overwrite

    def get_params(self):
        pass

    def add(self, params={}):
        """Add new attributes to self

        Parameters
        ----------
        params

        Returns
        -------

        """
        for i, (key, value) in enumerate(params.items()):
            setattr(self, key, value)


class DSP(Parameter):
    """ Dataset parameter (DSP)

    """

    def __init__(self, dataset_name='', **kwargs):
        """Common dataset parameter

        Parameters
        ----------
        dataset_name:

        """
        super(DSP, self).__init__()
        self.dataset_name = dataset_name
        case = 'new'
        self.direction = 'src'  # src: only source ip data; 'both': src+dst
        if case == 'new':  # new data
            # original_ipt_dir, only copy, should not be allowed to do any modification
            self.original_ipt_dir = "./datasets/reprst"  # new data (around 20201220)
            self.ipt_dir = "datasets/reprst_srcip"  # ipt_dir
            self.opt_dir = "./out/reprst_srcip"  # opt_dir
        else:
            # original_ipt_dir, only copy, should not be allowed to do any modification
            self.original_ipt_dir = "./datasets/reprst_srcip_old"  # old data (only src data before 20200601)
            self.ipt_dir = "datasets/reprst_srcip_old"  # ipt_dir
            self.opt_dir = "./out/reprst_srcip_old"  # opt_dir
        self.data_cat = 'INDV'  # data category: INDV, AGMT and MIX

        # quantile used to fix features' dimensions
        self.q_iat = 0.9
        self.sampling = 'rate'  # sampling method for SAMP-based features
        for i, (key, value) in enumerate(kwargs.items()):
            setattr(self, key, value)


class DTP(DSP):
    """Detector parameter (DTP)

    """

    def __init__(self, detector_name='', gs=True, **kwargs):
        """Commom detector parameters

        """
        super(DTP, self).__init__()
        self.detector_name = detector_name
        self.norm = True  # normalization or not
        self.norm_method = 'std'  # std
        self.gs = gs  # grid search to find the best parameters
        for i, (key, value) in enumerate(kwargs.items()):
            setattr(self, key, value)


class DPP(DTP):
    """Display parameter (DPP)

    """

    def __init__(self, title=True, **kwargs):
        """Commom display parameters

        """
        super(DPP, self).__init__()
        self.title = title
        for i, (key, value) in enumerate(kwargs.items()):
            setattr(self, key, value)


class EXPT(DPP):
    """All experimental parameters

    """

    def __init__(self, datasets=[], gses=[True, False], headers=[False, True], detector_name='GMM'):
        super(EXPT, self).__init__(detector_name=detector_name)

        #############################################################################################
        # 1.1 datasets
        self.datasets = datasets
        print(f'datasets: {self.datasets}')

        #############################################################################################
        # 1.2 Augmented data or not
        # 'AGMT' for UNB dataset, 'INDV' for the remained datasets
        self.data_cats = ['AGMT', 'INDV']
        print(f'data_cats: {self.data_cats}')

        #############################################################################################
        # 1.3 use subflow
        # default: True, split flows into subflows
        self.subflows = [True]
        # If self.subflow_interval==None, using self.q_flow_dur to obtain the self.subflow_interval in runtime and
        # split flows into subflows  (the detail can be found in data_factory.py).
        self.subflow_interval = None
        if True in self.subflows:
            if type(self.subflow_interval) == float and \
                    (self.subflow_interval > 0) and (self.subflow_interval < 10000) \
                    and (self.subflow_interval != None):
                print(f'+++subflow_interval ({self.subflow_interval}) is preset before the experiment.')
                self.q_flow_dur = None
            else:
                self.q_flow_dur = 0.90  # it's used for calculating the subflow interval in runtime
                print(f'subflow_interval ({self.subflow_interval}) will be calculated in runtime according '
                      f'to q_flow_dur ({self.q_flow_dur})')
            print(f'subflows: {self.subflows}, subflow_interval: {self.subflow_interval}, '
                  f'q_flow_dur: {self.q_flow_dur}')
        #############################################################################################
        # 1.4 Whether adding header information (such as TTL and TCP flags) into features or not
        self.headers = headers
        print(f'headers: {self.headers}')

        #############################################################################################
        # 2. detector
        self.detectors = [self.detector_name]
        # Whether using grid search or not.
        self.gses = gses

        #############################################################################################
        # 3. display

    def get_dataset_params(self):
        """ Get the common data parameter

        Returns:
            one combination of data parameter
        """
        # list(itertools.product(datasets, subflows, headers))
        for i_sf, subflow in enumerate(self.subflows):
            for i_hd, header in enumerate(self.headers):
                for i_dc, data_cat in enumerate(self.data_cats):
                    for i_ds, dataset_name in enumerate(self.datasets):
                        yield {'dataset_name': dataset_name, 'data_cat': data_cat, 'subflow': subflow, 'header': header}

    def get_detector_params(self):
        """ Get the common detector parameter

        Returns:
            one combination of detector parameter
        """
        for i, detector_name in enumerate(self.detectors):
            for j, gs in enumerate(self.gses):
                yield {"detector_name": detector_name, "gs": gs}

    def get_display_params(self):
        """ Get the display parameter

        Returns:
            one combination of display parameter
        """
        titles = [True]
        self.display_params_cnt = len(titles)
        for i, title in enumerate(titles):
            yield DPP(title=title)

    def get_one_comb(self):
        """ Get one combination of all parameters for each experiment

        Returns
        -------
            one combination of all parameters
        """
        for dsp in self.get_dataset_params():  # get dataset combinations
            for dtp in self.get_detector_params():  # get detector combinations
                self.add(dsp)
                self.add(dtp)
                # del params['datasets']
                # del params['data_cats']
                # del params['subflows']
                # del params['headers']
                # del params['detectors']
                # del params['gses']
                yield copy.copy(self.__dict__)  # shallow copy is enough

    def validate_comb(self, current_comb, num=0):
        """Get valid combination, especially for AGMT and INDV

        Parameters
        ----------
        current_comb
        num

        Returns
        -------

        """
        if (current_comb['dataset_name'] in [
            'UNB/CICIDS_2017/pc_192.168.10.5',
            'UNB/CICIDS_2017/pc_192.168.10.8',
            'UNB/CICIDS_2017/pc_192.168.10.9',
            'UNB/CICIDS_2017/pc_192.168.10.14',
            'UNB/CICIDS_2017/pc_192.168.10.15',

        ] and current_comb['subflow'] == True and current_comb['data_cat'] == 'AGMT'):
            valid = True
        elif (current_comb['dataset_name'] in [

            'CTU/IOT_2017/pc_10.0.2.15',

            'MAWI/WIDE_2019/pc_202.171.168.50',
            'MAWI/WIDE_2020/pc_203.78.7.165',
            # 'MAWI/WIDE_2020/pc_202.75.33.114',
            'MAWI/WIDE_2020/pc_23.222.78.164',
            'MAWI/WIDE_2020/pc_203.78.4.32',
            'MAWI/WIDE_2020/pc_203.78.8.151',
            'MAWI/WIDE_2020/pc_203.78.4.32',
            'MAWI/WIDE_2020/pc_203.78.4.32-2',
            'MAWI/WIDE_2020/pc_203.78.7.165-2',  # ~25000 (flows src_dst)

            'UCHI/IOT_2019/smtv_10.42.0.1',

            'UCHI/IOT_2019/ghome_192.168.143.20',
            'UCHI/IOT_2019/scam_192.168.143.42',
            'UCHI/IOT_2019/sfrig_192.168.143.43',
            'UCHI/IOT_2019/bstch_192.168.143.48'

        ] and current_comb['subflow'] == True and current_comb['data_cat'] == 'INDV'):
            valid = True
        else:
            valid = False

        if valid:
            num += 1

        return valid, num

    def get_combs(self):
        """Get total combinations and valid combinations

        Returns
        -------
            all combinations of experiments
        """
        self.num_dataset_combs = len(self.datasets) * len(self.data_cats) * len(self.subflows) * len(self.headers)
        print(f'num_dataset_combs: {self.num_dataset_combs} = {len(self.datasets)}(datasets)*'
              f'{len(self.data_cats)}(data_cats)*{len(self.subflows)}(subflows)*{len(self.headers)}(headers)')

        self.num_detector_combs = len(self.detectors) * len(self.gses)
        print(f'num_detector_combs: {self.num_detector_combs} = {len(self.detectors)}(detectors)*'
              f'{len(self.gses)}(gses)')

        self.num_combs = 0  # valid params combinations
        _combs = []
        for i, comb_i in enumerate(self.get_one_comb()):
            valid, self.num_combs = self.validate_comb(comb_i, self.num_combs)
            if not valid:
                continue
            else:
                _combs.append(comb_i)

        print(f'total combs: {self.num_dataset_combs * self.num_detector_combs}, valid combs: {self.num_combs}')
        self.combs = _combs

        return self.combs


@execute_time
@func_notation
def main(detector_name="GMM", start_time=time.strftime(TIME_FORMAT, time.localtime())):
    """Main function includes two steps:
        1. Get results by detector_name with all combinations of parameters
        2. Save the results to txt, and further transform them to csv, xlsx, and latex

    Parameters
    ----------
    detector_name : string
        outlier detection algorithm

    start_time:
        start time of the application

    """
    datasets = [
        # Naming: (department/dataname_year/device)
        'UNB/CICIDS_2017/pc_192.168.10.5',
        'UNB/CICIDS_2017/pc_192.168.10.8',
        'UNB/CICIDS_2017/pc_192.168.10.9',
        'UNB/CICIDS_2017/pc_192.168.10.14',
        'UNB/CICIDS_2017/pc_192.168.10.15',
        #
        'CTU/IOT_2017/pc_10.0.2.15',

        'MAWI/WIDE_2019/pc_202.171.168.50',
        # 'MAWI/WIDE_2020/pc_203.78.7.165', # ~25000 (flows src_dst)
        # 'MAWI/WIDE_2020/pc_23.222.78.164',    # 3 (big flows)
        # 'MAWI/WIDE_2020/pc_203.78.4.32',    # ~25000 (flows src_dst)
        # 'MAWI/WIDE_2020/pc_203.78.8.151',   # ~6600 (flows src_dst)
        # 'MAWI/WIDE_2020/pc_23.223.19.175',  # ~60 (flows src_dst)
        # 'MAWI/WIDE_2020/pc_114.234.20.197'    # 3 flows(src_dst)
        # 'MAWI/WIDE_2020/pc_202.75.33.114'  # 2000 flows(src)
        # 'MAWI/WIDE_2020/202.66.205.237'        # 580 (src)

        # # works
        # 'MAWI/WIDE_2020/pc_203.78.4.32',
        # 'MAWI/WIDE_2020/pc_203.78.4.32-2',
        # 'MAWI/WIDE_2020/pc_203.78.7.165-2',  # ~25000 (flows src_dst)

        # # # #
        'UCHI/IOT_2019/smtv_10.42.0.1',

        'UCHI/IOT_2019/ghome_192.168.143.20',
        'UCHI/IOT_2019/scam_192.168.143.42',
        'UCHI/IOT_2019/sfrig_192.168.143.43',
        'UCHI/IOT_2019/bstch_192.168.143.48'

    ]  # 'DEMO_IDS/DS-srcIP_192.168.10.5'
    gses = [False, True]
    headers = [False, True]
    #############################################################################################
    # 1. Obtain all combinations of experimental parameters
    expt = EXPT(datasets=datasets, detector_name=detector_name, gses=gses, headers=headers)
    combs = expt.get_combs()  # get valid parameters combinations.
    num_combs = expt.num_combs

    if not pth.exists(expt.opt_dir):
        os.makedirs(expt.opt_dir)

    # raw_txt_file = f'{expt.opt_dir}/{detector_name}_result_{start_time}.txt'
    raw_txt_file = f'{expt.opt_dir}/{detector_name}.txt'
    print(f"raw_txt_file: {raw_txt_file}")
    # save all experimental results to disk
    results = OrderedDict()

    #############################################################################################
    # 2. Stored processed_pcaps to avoid parsing the same pcap again
    processed_pcaps = []
    for i, current_comb in enumerate(combs):
        try:
            progress_bar(i, num=num_combs)
            if current_comb['dataset_name'] in processed_pcaps:
                current_comb['overwrite'] = False
            else:
                # default True. False: use the previous flows and labels extracted from pcaps.
                current_comb['overwrite'] = False
                processed_pcaps.append(current_comb['dataset_name'])

            # different algorithms have their own datasets to avoid access issues caused by different algorithms
            # simultaneously
            # current_comb will be updated, such as in_dir and out_dir in every loop.
            current_comb = copy_dataset(current_comb)
            # if i == 0:
            #     # remove old file to get more space
            #     rm_outdated_files(current_comb['ipt_dir'], dur=7 * (24 * 60 * 60))
            #     rm_outdated_files(current_comb['opt_dir'], dur=7 * (24 * 60 * 60))

            #############################################################################################
            # 2.1 obtain results with each combination of parameters, which includes three steps:
            # 1) data process 2) build and evaluate the ndm 3) display and save the resutls to files
            mf = MainFactory(params=current_comb)
            current_result = mf.run()

            #############################################################################################
            # 2.2 get key and save current results to all results
            dt = current_comb.get('detector_name')
            gs = current_comb.get('gs')
            sf = str(current_comb.get('subflow')) + '-q_flow_dur:' + str(current_comb.get('q_flow_dur')) \
                 + '-interval:' + str(current_comb.get('subflow_interval'))
            hdr = current_comb.get('header')
            dc = current_comb.get('data_cat')
            ds = current_comb.get('dataset_name')
            # keys list:
            keys = [dt, f'gs:{gs}', f'sf:{sf}', f'hdr:{hdr}', dc, ds]
            print(f'{i}, keys:{keys}')
            # mf.data_lst already decides the order of aucs
            # save current experimental results to all results, i.e., update the OrderedDict of the results
            set_dict(results, keys=keys, value=current_result)

            # Although saving results everytime does redundant work, it can check results immediately
            # and avoid missing out of any data due to any unknown reason (e.g., out of memory, which can't be
            # captured by try-catch statements).
            save_total_results(copy.deepcopy(results), output_file=raw_txt_file, detector_name=detector_name,
                               params=current_comb)

            # print(f'{i}, current results: {current_result}')
            del mf  # delete mf to release memory
        except (MemoryError, KeyboardInterrupt, Exception) as e:
            print(f"{i}/{num_combs},{e}")
            traceback.print_tb(e.__traceback__)  # cann't be saved to txt; otherwise it is given a file parameter
            continue
        except:  # can catch any exceptions
            print(f"{i}/{num_combs}, unexpected error: {sys.exc_info()}")
            continue
    # print the last progress bar
    progress_bar(i + 1, num=num_combs)  # all experiments are conducted.
    print('finished.')

    return 0


def main_demo():
    for detector in ['IF']:  # ['GMM', 'PCA']
        print(f'\n\n***: {detector}')
        args = parse_cmd_args(detector)
        main(detector_name=args.detector.upper(), start_time=args.time)
        # ####################################################################################################
        # # in parallel
        # # get the number of cores. Note that, one cpu might have more than one core.
        # n_jobs = int(joblib.cpu_count() // 8) or 2
        # print(f'n_job: {n_jobs}')
        # parallel = Parallel(n_jobs=n_jobs, verbose=30)
        # gses = [False, True]
        # headers = [False, True]
        # with parallel:
        #     outs = parallel(
        #         delayed(main)(detector_name=args.detector.upper(), gs=gs, header=header, start_time=args.time)
        #         for (gs, header) in itertools.product(gses, headers))


@func_notation
def parse_cmd_args(detector='GMM'):
    """Parse commandline parameters

    Returns:
        args: parsed commandline parameters
    """
    parser = argparse.ArgumentParser()
    parser.add_argument("-d", "--detector", help="outlier detection algorithm", default=detector)
    parser.add_argument("-t", "--time", help="start time of the application",
                        default=time.strftime(TIME_FORMAT, time.localtime()))
    args = parser.parse_args()
    print(f"args: {args}")

    return args


if __name__ == '__main__':
    # use reprst2neon.sh
    args = parse_cmd_args()
    main(detector_name=args.detector.upper(), start_time=args.time)
